ResNet50
ResNet이 나오기 전까지 많은 모델들은 정확도를 높이기 위해 단순히 레이어를 많이 쌓아 깊은 모델을 만들어냈다. 하지만 모델이 깊어질수록 곱해지는 미분값이 증가함에 따라 초기 E 값 자체에 집중을 못하기 때문에 효율과 최적화가 떨어졌는데, 이를 vanishing gradient problem이라고 한다. 한마디로, 모델이 깊어질수록 기울기의 영향력이 줄어드는 현상이다. 
ResNet에 사용되는 Residual Block는 모델에 사용되는 가장 기본적인 구조이다. 이 구조에 사용되는 residual connection은 레이어와 레이어 사이를 건너뛰는 일종의 지름길에 해당하는 연결이다. 입력값이 곧바로 출력값에 들어가기 때문에 레이어가 많아지면서 입력값이 잊혀지는 vanishing gradient problem을 해결할 수 있다.
ResNet50은 residual block을 사용하여 총 50개의 계층으로 구성된 컨벌루션 신경망이다. 레이어마다 다른 residual block 형태가 반복되어 학습되는 과정을 거친다.

출처: Deep Residual Learning for Image Recognition

VGG16
VGG16은 ResNet50과는 다르게 신경망이 깊고 3*3의 컨벌루션 레이어로 이루어져 있다. 과거 모델들은 큰 receptive field를 갖는 11*11이나 7*7 필터를 사용했지만 VGG는 3*3 필터를 사용하여 이미지 분류 정확도를 개선시켰다. 또한 3*3 필터로 컨벌루션 연산을 하면서 ReLU 연산이 3번 적용되기 때문에 비선형 함수가 더 많이 적용된다. 때문에 레이어가 증가할수록 비선형성이 증가하여 모델 특징에 식별성이 증가한다. 또한 3*3 필터를 사용하기 때문에 파라미터가 감소한다. 

출처: Very deep convolutional networks for large-scale image recognition